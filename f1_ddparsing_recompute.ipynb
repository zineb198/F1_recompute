{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDP_parsing prediction and f1 score with published model \n",
    "\n",
    "Model downloaded from https://drive.google.com/file/d/1ytLubJSWP1eeZDpQekWkWmJ1D0ODCvou/view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "map_relations = {'Comment': 0, 'Contrast': 1, 'Correction': 2, 'Question-answer_pair': 3, 'QAP': 3, 'Parallel': 4, 'Acknowledgement': 5,\n",
    "            'Elaboration': 6, 'Clarification_question': 7, 'Conditional': 8, 'Continuation': 9, 'Result': 10, 'Explanation': 11,\n",
    "            'Q-Elab': 12, 'Alternation': 13, 'Narration': 14, 'Background': 15, 'Break': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 3, 3, 5]\n",
      "[[1, 0], [2, 1], [3, 1], [4, 3]]\n"
     ]
    }
   ],
   "source": [
    "pred_file =  open(\"outputs/stac_predictions_attach.pkl\",'rb')\n",
    "stac_predictions = pickle.load(pred_file)\n",
    "pred_rel_file =  open(\"outputs/stac_predictions_relation.pkl\",'rb')\n",
    "stac_rel_predictions = pickle.load(pred_rel_file)\n",
    "f = open(\"data/stac_data/test_data.json\")\n",
    "stac_test = json.load(f)\n",
    "\n",
    "print(stac_rel_predictions[0])\n",
    "print(stac_predictions[0])\n",
    "\n",
    "# delete the root node from the predictions and regroup\n",
    "test_stac_pred = [[(p[1]-1,p[0]-1) for p in dialog_pred[1:]] for dialog_pred in stac_predictions if len(dialog_pred) >1]\n",
    "test_stac_pred_rel = [[(stac_predictions[j][i][1]-1,stac_predictions[j][i][0]-1,stac_rel_predictions[j][i]) for i in range(1,len(stac_predictions[j]))] for j in range(len(stac_predictions)) if len(stac_predictions[j]) >1]\n",
    "\n",
    "test_stac_truth = []\n",
    "for dialogue in stac_test:\n",
    "    truth = []\n",
    "    if len(dialogue['edus'])==1:\n",
    "        continue\n",
    "    for edu in dialogue['relations']:\n",
    "        tup = (edu['x'],edu['y'],map_relations[edu['type']])\n",
    "        truth += [tup]\n",
    "    test_stac_truth += [truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of correct predictions :  810\n",
      "nb of attachments in the gold data :  1243\n",
      "nb of predictions :  1154\n",
      "F1 score : 0.6758448060075093\n",
      "F1 score 1 parent per edu : 0.7019064124783362\n"
     ]
    }
   ],
   "source": [
    "# f1 for attachments - code for f1 computation from shi and huang \n",
    "# cnt_pred\n",
    "cnt_pred = []\n",
    "for dialog in test_stac_pred:\n",
    "    cnt_pred += [len(dialog)+1]\n",
    "cnt_pred = sum(cnt_pred)\n",
    "\n",
    "# cnt_golden\n",
    "cnt_golden = []\n",
    "for i,dialog in enumerate(stac_test):  \n",
    "    cnt_g = len(dialog['relations'])\n",
    "    cnt = [0] * len(dialog[\"edus\"])\n",
    "    for r in dialog[\"relations\"]:\n",
    "        cnt[r[\"y\"]] += 1\n",
    "    for j in range(len(dialog[\"edus\"])):\n",
    "        if cnt[j] == 0:\n",
    "            cnt_g += 1\n",
    "    cnt_golden += [cnt_g]\n",
    "cnt_golden = sum(cnt_golden)\n",
    "\n",
    "cnt_correct = []\n",
    "for i, dialog_pred in enumerate(test_stac_pred):\n",
    "    val = 0\n",
    "    truth = [(p[0], p[1]) for p in test_stac_truth[i]]\n",
    "    for pred in dialog_pred : \n",
    "        if pred in truth:\n",
    "            val += 1\n",
    "    cnt_correct += [val+1]\n",
    "cnt_correct = sum(list(cnt_correct))\n",
    "print('nb of correct predictions : ',cnt_correct)\n",
    "print('nb of attachments in the gold data : ', cnt_golden)\n",
    "print('nb of predictions : ', cnt_pred)\n",
    "precision = cnt_correct*1.0/cnt_pred*1.0\n",
    "recall = cnt_correct*1.0/cnt_golden*1.0\n",
    "\n",
    "f1 = 2*( precision * recall / (precision + recall))\n",
    "print('F1 score :', f1)\n",
    "\n",
    "\n",
    "# cnt_golden-cnt_pred = 89\n",
    "precision = cnt_correct*1.0/cnt_pred*1.0\n",
    "recall = cnt_correct*1.0/(cnt_golden-89)*1.0\n",
    "f1_bis = 2*( precision * recall / (precision + recall))\n",
    "print('F1 score 1 parent per edu :', f1_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of correct predictions :  584\n",
      "nb of attachments in the gold data :  1243\n",
      "nb of predictions :  1154\n",
      "F1 score : 0.4872757613683771\n",
      "F1 score 1 parent per edu : 0.5060658578856152\n"
     ]
    }
   ],
   "source": [
    "# f1 for relations - code for f1 computation from shi and huang \n",
    "# cnt_pred\n",
    "cnt_pred = []\n",
    "for dialog in test_stac_pred:\n",
    "    cnt_pred += [len(dialog)+1]\n",
    "cnt_pred = sum(cnt_pred)\n",
    "\n",
    "# cnt_golden\n",
    "cnt_golden = []\n",
    "for i,dialog in enumerate(stac_test):  \n",
    "    cnt_g = len(dialog['relations'])\n",
    "    cnt = [0] * len(dialog[\"edus\"])\n",
    "    for r in dialog[\"relations\"]:\n",
    "        cnt[r[\"y\"]] += 1\n",
    "    for j in range(len(dialog[\"edus\"])):\n",
    "        if cnt[j] == 0:\n",
    "            cnt_g += 1\n",
    "    cnt_golden += [cnt_g]\n",
    "cnt_golden = sum(cnt_golden)\n",
    "\n",
    "cnt_correct = []\n",
    "for i, dialog_pred in enumerate(test_stac_pred_rel):\n",
    "    val = 0\n",
    "    truth = [(p[0], p[1], p[2]) for p in test_stac_truth[i]]\n",
    "    for pred in dialog_pred : \n",
    "        if pred in truth:\n",
    "            val += 1\n",
    "    cnt_correct += [val+1]\n",
    "cnt_correct = sum(list(cnt_correct))\n",
    "print('nb of correct predictions : ',cnt_correct)\n",
    "print('nb of attachments in the gold data : ', cnt_golden)\n",
    "print('nb of predictions : ', cnt_pred)\n",
    "precision = cnt_correct*1.0/cnt_pred*1.0\n",
    "recall = cnt_correct*1.0/cnt_golden*1.0\n",
    "\n",
    "f1 = 2*( precision * recall / (precision + recall))\n",
    "print('F1 score :', f1)\n",
    "\n",
    "precision = cnt_correct*1.0/cnt_pred*1.0\n",
    "recall = cnt_correct*1.0/(cnt_golden-89)*1.0\n",
    "f1_bis = 2*( precision * recall / (precision + recall))\n",
    "print('F1 score 1 parent per edu :', f1_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dparsing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb6a3abd8efab72d05d5f022ef957bf5193fb57ce0e10c14109d6a4dcfb8dec0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
